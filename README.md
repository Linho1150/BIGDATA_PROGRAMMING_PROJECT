명지대학교 인문캠퍼스

핵심 버스노선 교통흐름 분석

레포트 정보
===========

학교 : 명지대학교

학기 : 2019년 2학기

과목명 : 빅데이터 프로그래밍

지도교수 : 권동섭 교수님

작성자 : 명지대학교 경영정보학과 60171918 이재준

\[문제정의\] 배경
=================

명지대학교 (인문캠퍼스)의 교통 특성상 가장 많이 사용되는 대중교통은
버스이다. 하지만 버스가 이용하는 차선은 왕복 1차선밖에 되지 않는다. 이는
자연스럽게 많은 교통체증을 약이 한다. 그만큼 교통 혼잡이 많이 일어나고
작성자 또한 학생 중 한 명으로서 다른 학생에게 빈번하게 들은 말 중 한
가지가 "지금 버스 오래 걸릴까?" 이다. 이런 상황을 보안을 유지하기 위해
셔틀버스를 운행하나 한정적인 좌석과 고정된 배차 시간으로 인해 불편을
겪고 있다. 결과적으로 명지대학교 학생은 마을버스를 주 대중교통으로
이용하고 있다.

\[문제정의\] 목적
=================

앞서 얘기한 배경을 사유로 버스 배차 간격을 시간대별로 분석하여 가장
교통이 혼잡한 시간대와 혼잡하지 않은 시간대를 나눠보려고 한다. 첫 번째로
명지대학교 정문 방향의 정류장\[명지대(13195)\]과 이삭 토스트 명지대점
앞의 정류장 \[명지대(13194)\] 2가지의 변화하는 버스 배차 간격을
조사하고, 전체적인 버스 배차 간격의 평균을 함께 조사할 것이다. 본
레포트는 교통 흐름의 특성상 차가 많을수록 버스의 배차 간격은 점점
늘어난다는 사실을 전제로 조사하게 되었다.

\[문제정의\] 목표
=================

버스가 운영하는 시간을 1시간 단위로 나누어 배차 간격이 짧은 시간대부터
긴 시간대를 파악하고 특정 버스를 어떤 시간대에 피해야 하는지 파악한다.
결과값을 가시화 프로그램을 통해 전달하는 것이 최종 목표이다.

\[문제정의\] 사전조사
=====================

![](.//media/image1.png)\[명지대(13195)\]와 \[명지대(13194)\]
정류장의 정보를 먼저 조사하였다.

위 그림1의 빨간색은 정류장 \[명지대 13195\], 파란색은 \[명지대 13194\]
정류장이며 녹색으로 표시된 부분이 명지대학교 인문캠퍼스의 행정동
건물이다.

정류장 \[명지대 13195\]는 버스 노선 7017, 7019, 7021, 7611, 7612, 7713,
7734가 운행 중이다. (2019.12.16 기준)

정류장 \[명지대 13194\]는 버스 노선 7017, 7019, 7021, 7611, 7612, 7713,
7734 가 운행 중이다. (2019.12.16 기준)

시스템 아키텍처
===============

![](.//media/image3.png)전체적인 시스템의 흐름은 아래 그림3과
동일하다.

1.  m.bus.go.kr을 크롤링할 웹 크롤러를 24시간 가동하였다.

    A.  ![](.//media/image4.png)Amazon EC2에 Ubuntu를 사용하여
        Python으로 만들어 놓은 웹 크롤러를 Linux의 Nohub(데몬)을
        사용하여 가동하였다. (그림 4 참고)

    B.  ![](.//media/image6.png)일반 컴퓨터에 Windows 10을 설치하고
        Python으로 제작한 웹 크롤러를 사용해 가동하였다. (그림 5 참고)

2.  웹 크롤러를 통해 Amazon RDS의 Mysql에 접속하여 데이터 SELECT 와
    INSERT를 병행하였다.

    C.  앞서 크롤링과 데이터 입력이 끝나면 쉴 틈없이 Python을 통해 다시
        크롤링 하길 무한 반복하였다.

    D.  m.bus.go.kr에서 크롤링을 성공하고 변동 여부가
        파악되면Python에서는 Amazon RDS의 Mysql로 현재 크롤링한 버스와
        정류장의 이전 시간 정보를 가져오고 현재 시간을 로드하여 차이
        값을 계산한다. 이는 배차간격을 뜻한다.

    E.  웹 크롤러에 대한 세부 내용은 아래 "PYTHON 웹크롤러 아키텍처"를
        확인하면 된다.

3.  Aamazon RDS에 저장 되어있는 데이터를 Sqoop을 통해 Data Warehouse인
    Hadoop hdfs에 저장한다.

    F.  Sqoop에서 Hdfs로 데이터를 로드 하는 과정에서 Hive를 함께
        설정하여 Hdfs로 데이터 전송이 완료되면 지정해 놓은 Hive의
        Database로 값이 Insert 된다.

4.  전송된 Hive값을 바탕으로 Zeppelin을 통해 Visualization 한다.

왜 여러 개의 크롤러를 구동 시키는가?
====================================

시스템 아키텍처를 보면서 들 수 있는 가장 큰 의문이다. 한 개의 웹
크롤러를 이용하여 여러 개의 버스 노선 정보를 가져오면 되는 것이 아닌지
생각할 수 있다. 이번 프로젝트에서 웹 크롤러를 2개 사용한 이유는 생각보다
단순하다. 1초의 시간이 상당히 중요했기 때문이다. 다른 노선의 정류장
정보를 크롤링 하는 순간에도 시간은 흐른다. 그렇게 되면 해당하지 않는
노선의 버스 정류장은 버스의 유무가 바뀌더라도 신경 쓸 수 없게 된다.
사실, 5개의 노선을 크롤링하는 과정은 문제가 없다. 하지만 초기 프로젝트
규모로 생각해보면 납득이 된다. 본래 현 프로젝트는 서울을 지나치는 모든
노선(약 650개)의 정보를 크롤링하려고 하였다. 650개의 프로세스를 한번에
크롤링 하는 것은 배차간격의 오차범위가 계속 늘어났다. 물론
"비동기"방식을 이용해서도 시간은 줄일 수 있지만, Amazon RDS의 Mysql을
사용하여 비동기까지 처리하는 방식은 리스크가 굉장히 컸다. 비동기에 여러
개의 Thread 까지 사용하는 방법은 최고의 해결 방법이었으나 실력의 한계로
1초라도 정확도를 올려보고자 다른 컴퓨터를 사용하여 맡은 버스 노선을
크롤링하게 된 것이다. 어떤 Job이 먼저 실행되고 처리될지 보장되지 않은
상황에 모든 방식을 비동기화 시키게 된다면 중복 데이터 값부터 불필요한
계산까지 요구하게 된다. 최악의 경우 알 수 없는 오류까지도 장담할 수 없는
것이다. 따라서 현 프로젝트에서 웹 크롤링은 비동기 방식으로 시도했으나
Mysql의 데이터를 SELECT 하고 INSERT 하는 과정은 사실상 "동기"처리로
진행하였다.

참고 : 그림4, 그림 5

왜 여러 개의 DATABASE를 구동 시키는가?
======================================

크롤러는 그럴 수 있어도 데이터베이스는 함께 쓸 수 있지 않을까? 생각할 수
있다. 이는 현재의 프로젝트 보다 더 큰 규모를 생각하였을 때를 위해
구현하게 되었다. 서울시만 봐도 총 653개의 버스 노선이 있다. 서울시를
지나치는 버스 노선의 개수다. 이러한 노선이 지역별로 존재한다. 더 큰
규모로 보았을 때 나라별로 존재하게 된다. 막연하게 큰 규모지만 해당
규모를 생각하며 진행하게 되었다.

\[데이터 수집 방법\] m.bus.go.kr 정보
=====================================

먼저, 정류장 \[명지대(13195)\]와 정류장 \[명지대(13194)\]를 "명지대
정류장"로 명시하며 버스정류장 정보는
[[http://m.bus.go.kr/]{.underline}](http://m.bus.go.kr/) 에서
크롤링하기로 하였다. 해당 사이트는 서울의 대중교통을 종합적으로 나타낸
페이지이다. 정확하고 간편한 웹 크롤링을 위해 모바일 웹사이트를 타깃으로
지정하였다. 사이트의 구체적인 모습을 보도록 하자.

![](.//media/image8.png)

그림 5 \_ m.bus.go.kr 버스노선 정보

위의 그림2는 실제 m.bus.go.kr에서 제공하는 버스 노선별 상황이며 이를
예시로 설명하겠다. 현재 버스는 정류장 \[서울중앙교회앞 12417\] 노선 위에
표시되어있다. 이 의미는 버스는 정류장 \[서울중앙교회앞 12417\]에 이미
도착했거나 지나서 정류장 \[은가어린이공원구150번종점\]을 향하고 있다는
뜻이다. 결론적으로 노선 위의 버스 아이콘이 다음 정류장으로 넘어가면
동시에 값을 해당 시간을 체크하여 해당 정류장에 버스가 도착한 시간을 알
수 있다. 이와 같은 방법을 사용하여 웹 크롤러를 지속해서 반복시키고
해당하는 값이 변경되었을 때 버스 번호, 정류장 번호, 시간을 체크하였다.
해당 파일의 데이터를 2019년 11월 24일부터 2019년 12월 15일까지 총 22일간
축적했고 시간대별로 배차 시간을 분석하였다.

\[데이터 수집 방법\] PYTHON 웹크롤러 아키텍처
=============================================

![](.//media/image9.png)아래 그림은 Python으로 제작한 웹 크롤러의
전체적인 아키텍처이다.

\[데이터 수집 방법\] PYTHON 웹 크롤러 코드
==========================================

\[크롤링할 버스노선 가져오기\] Buslist.txt에 있는 버스 노선의 정보를
Enter line을 기준으로 읽어온다.

\[버스 번호별 m.bus.go.kr 크롤링\]\[버스 번호에 해당하는 id 가져오기\]
버스 번호를 입력 받고 버스 번호에 해당하는 버스 id 값을 가져오는
부분이다. 해당 주소에 버스 번호로 데이터를 요청하면 json 형태로 값을
출력한다. 출력된 값의 resultLis\[0\] busRouteld 의 값을 출력한다.

\[버스 노선별 m.bus.go.kr 크롤링\]\[크롤링 Json 데이터 분리\]\[특정
버스번호, 버스정류장 선택\] 해당하는 버스 번호의 id값을 입력 받고 입력
받은 버스 id의 전체 정류장과 정류장에 해당하는 버스의 유무를 파악한다.
stationNo는 정류장 번호이고 existYn은 버스가 해당 정류장에 있으면 Y를
출력하고 없으면 N을 출력한다.

\[기타\] 프로그램 실행 중에 발생하는 오류를 Log파일에 저장해주는
함수이다. 오류가 발생한 시간과 오류명을 함께 ERROR\_LOG.txt 파일에
저장한다.

\[해당 버스정류소에 해당 버스 유무의 변동이 있는가?\] \[모든 버스번호와
정류장 번호를 로드하였는가?\] \[다음 버스 번호, 정류장 번호 로드\]
프로세스가 시작되면 들어오는 main 함수이다. 먼저 메모장에서 읽어온 버스
번호 리스트를 busnumberToid 함수에 넣어 id로 출력한다. Gather은 result에
busnum을 전부 담을 때까지 기다려야함으로 사용한 함수이다.

result에서 가져온 bus 값은 정류장번호와 버스 유무가 포함되어 있다.
str\_before\_exist는 mysql\_search\_data 함수와 연결 되어있는데,
mysql\_search\_data는 버스번호와 정류장 번호를 넘겨주어 이전의 BUS\_IS
값이 Y 인지 N인지 판단한다.

이전의 BUS\_IS 값이 N라고 가정하고 이번 값이 Y가 들어오면 해당 정류장의
버스가 도착했음으로 값을 mysql\_insert\_data 함수를 통해 저장한다.

\[현재시간과 최근 방문 시간의 차이를 분석 후 WTIME으로 선언\] \[버스
번호, 정류장 번호, 버스 유무, WTIME을 MYSQL에 insert\]
msql\_insert\_data 함수는 데이터 변동이 확인되었을 때 실행되는 함수이다.
해당 정류장의 버스가 도착했거나 떠났음으로 이를 알리기 위해 데이터를
추가하는 것이다.

\#시간시작의 str\_before\_time은 mysql\_get\_time\_data 함수와 연결
되어있다. 해당 함수는 데이터를 변경할 정류장의 데이터를 변경할 버스
노선이 현재를 제외하고 최근에 언제 들어왔는지 시간을 파악해준다. 따라서
나온 날짜 값과 현재 날짜 값의 차이를 구해서 초 단위로 얼마의 시간이
흘렀는지 계산한다.

\#MYSQL insert는 Mysql에 규격에 맞춰 데이터를 Insert 하게 된다.

\[기타\] Mysql\_get\_time\_data의 사용방법은 앞의 mysql\_insert\_data
함수를 설명하는 과정에 간단한 아키텍처를 설명하였다.

\[해당 버스정류장에 해당 버스 최근 방문 시간을 Mysql에서 select\]
Mysql\_search\_data는 가장 최근에 BUS\_IS의 값이 Y 이었는지 N이었는지
확인하는 단계이다. 이를 확인해서 현재 불러온 값이 Y이고 이전의 값이
N였다면 데이터를 갱신해주기위 해서 mysql\_insert\_data 함수를 실행하게
되고 값이 같다면 변동이 없다는 뜻으로 별다른 코드를 실행시키지 않고 다시
웹크롤링 과정에 돌입하게 된다.

\[데이터 분석 방법\] Amazon rds - mysql schema
==============================================

**CREATE** **TABLE** \`BUS\_LIVE\` (

\`INDEX\_NUM\` **BIGINT**(20) **NOT** **NULL**,

\`BUS\_ID\` **VARCHAR**(50) **NOT** **NULL**,

\`STATION\_ID\` **VARCHAR**(50) **NOT** **NULL**,

\`TIME\` **DATETIME** **NOT** **NULL**,

\`BUS\_IS\` **VARCHAR**(50) **NOT** **NULL**,

\`WTIME\` **BIGINT**(20) **NULL** **DEFAULT** **NULL**,

**PRIMARY** **KEY** (\`INDEX\_NUM\`, \`BUS\_ID\`, \`STATION\_ID\`,
\`TIME\`)

)

**COMMENT**=\'실시간 버스 정보\'

**COLLATE**=\'utf8\_general\_ci\'

**ENGINE**=**InnoDB**

;

위의 SQL문은 BUS 정보를 데이터베이스인 BUS\_LIVE의 실제 스키마 정보를
나타낸 SQL문이다.

INDEX\_NUM : 가장 최근의 값을 가져오기 위해 설정하였다. 버스 번호, 버스
정류장 리스트를 한번 모두 반복하면 INDEX\_NUM의 값이 +1 된다.

BUS\_ID : 버스 번호의 정보를 담고 있다.

STATION\_ID : 정류장 번호의 정보를 담고 있다.

TIME : 데이터가 입력된 시간 정보를 담고 있다.

BUS\_IS : BUS\_ID의 버스가 STATION\_ID 정류장에 있으면 "Y"를 입력,
없으면 "N"를 입력 받는다.

WTIME : 배차 간격을 뜻한다. BUS\_ID의 버스가 STATION\_ID 정류장에 이전에
도착한 버스기록과 현 row가 등록된 시간이 얼마만큼의 간격을 갖고 있는지
초 단위로 저장한다.

PRIMARY KEY는 INDEX\_NUM, BUS\_ID, STATION\_ID, TIME을 묶었다.
검색속도가 빨라지는 경우 두 번 값이 입력될 수 있기에 INDEX\_NUM 또한
PRIMARY KEY로 묶었다.

\[데이터 분석 방법\] Amazon rds - mysql 전처리 예시
===================================================

![](.//media/image19.png)

그림 7\_AmazonRDS의 실제MYSQL데이터

위 그림7은 데이터가 담겨있는 실제 테이블이다. 왼쪽부터 INDEX\_NUM,
BUS\_ID, STATIOIN\_ID, TIME, BUS\_IS, WTIME 순이다. 컬럼별로 입력되는
데이터 값의 규칙은 아래 예시와 같다.

INDEX\_NUM,BUS\_ID,STATION\_ID,TIME,BUS\_IS,WTIME

**200000,7019,13195,2019-12-16 07:24:12,Y,95**

이를 해석하자면 200000번째에 크롤링해본 결과이고, 7019 버스는
13195정류장에 2019년 12월 16일 07시 24분 12초에 도착하였다(BUS\_IS="Y").
7019버스는 13195 정류장에 이전 버스가 출발한지 95초만에 다시 왔다.

\[데이터 분석 방법\] sqoop 설정
===============================

Sqoop을 설정하는 과정에서 크게 신경 써야 하는 부분은 총 2가지였다.
첫번째는 Database를 불러오면서 Hive에도 업로드 되어야 하는 것. 두번째는
기존의 데이터에서 중복되는 데이터 처리 방법. 따라서 처음 실행할 때 Sqoop
코드와 이후 Sqoop을 나눠서 항상 처리하였다. 두 코드가 별 차이 없지만
덮어쓰기 오류를 발생시키지 않는 큰 역할을 하였다.

\[ Sqoop 첫 실행 \]
-------------------

sqoop import \--connect
jdbc:mysql://bus-live.canb61sq58tb.ap-northeast-2.rds.amazonaws.com:3306/bus-live
\\

\--username admin \\

\--password ***** \\

\--table BUS\_LIVE \\

\--target-dir /user/maria\_dev/main\_bus \\

\--hive-import \\

\--hive-home /user/maria\_dev/main\_bus \\

\--hive-overwrite \\

\--create-hive-table \\

\--hive-table buslive.seoul

Amazon RDS의 주소와 Mysql의 기본 포트 3306을 설정해 놓았다. /bus-live는
Mysql의 데이터베이스 명이다. --table은 데이터베이스 bus-live에 있는
테이블 명이며 --target-dir은 해당 데이터베이스를 저장할 hdfs의 경로를
고정으로 지정해 놓았다. 이후 Hive에 데이터를 넣기 위해 --hive-import를
사용하였으며 Hive에 들어갈 데이터 또한 hdfs와 같은 경로에 저장해 놓았다.
혹시 모를 경우를 대비하여 덮어쓰기를 허용해 놓았고 Hive에 Database만
설정해 놓은 후 새로운 Table은 생성해 놓지 않은 상태이기 때문에
--create-hive-table로 데이터를 생성해 놓았다. 마지막으로 --hive-table
buslive.seoul로 Hive의 데이터베이스 명이 buslive임을 밝히고 희망하는
table 명을 seoul로 설정하였다.

\[ Sqoop 재실행 (데이터 덮어쓰기) \]
------------------------------------

sqoop import \--connect
jdbc:mysql://bus-live.canb61sq58tb.ap-northeast-2.rds.amazonaws.com:3306/bus-live
\\

\--username admin \\

\--password ***** \\

\--table BUS\_LIVE \\

\--delete-target-dir \\

\--target-dir /user/maria\_dev/main\_bus \\

\--table BUS\_LIVE \\

\--hive-import \\

\--hive-home /user/maria\_dev/main\_bus \\

\--hive-overwrite \\

\--hive-table buslive.seoul

위 Sqoop 처음 실행 코드와 큰 차이는 없다. 달라진 점은 target-dir을
설정하기 앞서 데이터가 덮어쓰기 되지 않기 때문에 delete-target-dir으로
target-dir의 값을 제거하였다. 그리고 테이블이 이미 생성되어 있기 때문에
--create-hive-table명령어를 삭제하고 --hive-overwrite명령어만
유지시켰다. (기존에 --create-hive-table을 제거하지 않아 오류가
지속적으로 발생하였다.)

\[데이터 분석 방법\] oozie 설정
===============================

AMAZON RDS MYSQL \> HDFS \> HIVE 과정을 자동화 하기 위해 Oozie의 Xml
파일을 만들고 Coordinate를 제작하였다. Day 1을 기준으로 하루에 한번
실행될 수 있도록 하였다. Xml의 Command는 위 Sqoop에서 재실행 부분을 넣어
놓았다.

![](.//media/image20.png)

그림 8\_OOZIE DASH BOARD

그림 9\_coordinator xml + exec shell xml

본 프로젝트에서 OOZIE의 아키텍처는 Coordinator 가 이벤트를 실행시킬
기준을 갖고 일정 시간에 워크플로우를 실행시키며 시간이 되었을 때 Shell을
통해 Sqoop import 가 담겨있는 xml 파일을 실행시키게 된다.

그림 10\_sqoop\_import\_xml

\[데이터 분석 방법\] Zeppelin 설정
==================================

Visualization을 위해서 Zeppelin을 설정하였다. 위 SQOOP을 통해 Hive에
Database를 Import 한 걸 참고하면 쉽게 이해할 수 있다.

1.  정류장 \[명지대 (13195)\]의 시간별 배차간격 결과를 출력하기 위해
    아래와 같은 SQL문을 zeppelin에 추가하였다.\
    \
    %jdbc(hive)\
    SELECT bus\_id,AVG(wtime)/60 AS WAITTIME,HOUR(time) AS HOURTIME\
    FROM buslive.seoul WHERE station\_id=\'13195\' AND WTIME\<5000 AND
    BUS\_IS=\'Y\' GROUP BY BUS\_ID,HOUR(time);

> 13195정류장의 버스 노선, 배차간격, 시간대를 정렬하였으며 시간대를
> 그룹으로 설정하여 시간대별로 버스 노선과 평균 대기시간을 출력하였다.\
> \
> 참고로 bus\_is ='Y'는 버스가 해당 정류장에 도착했을 때 시간을 갖고
> 오기 위해 설정하였다. 또한 wtime \< 5000 을 적용한 이유는 24시간
> 돌아가는 크롤러 특성상, 막차에서 첫차까지 운영되지 않을 때의 간격이
> 배차간격으로 입력된다. 첫차-막차 간격이 평균적으로 5000이 넘음으로
> 이를 조건에서 제외시키기 위해 5000이라는 숫자를 입력하였다.

2.  정류장 \[명지대 (13194)\]의 시간별 배차간격 결과는 위와 동일한
    방법이지만 station\_id='13195' 항목을 station\_id='13194'로
    변경하였다\
    \
    %jdbc(hive)\
    SELECT bus\_id,AVG(wtime)/60 AS WAITTIME,HOUR(time) AS HOURTIME\
    FROM buslive.seoul WHERE station\_id=\'13194\' AND WTIME\<5000 AND
    BUS\_IS=\'Y\' GROUP BY BUS\_ID,HOUR(time);\
    \
    위의 SQL문의 소요시간은 23분 36초이다.

3.  특정 정류장이 아닌 모든 정류장의 평균 배차간격 결과는 위와 동일한
    방법이지만 station\_id의 조건을 제거하였다.\
    \
    %jdbc(hive)\
    SELECT bus\_id,AVG(wtime)/60 AS WAITTIME,HOUR(time) AS HOURTIME FROM
    buslive.seoul WHERE WTIME\<5000 AND BUS\_IS=\'Y\' GROUP BY
    BUS\_ID,HOUR(time);

4.  출력된 그래프는 Keys를 hourtime으로, Groups를 bus\_id 로 그리고\
    Values를 waittime으로 설정하였다.

\[데이터 분석 결과\] 분석 데이터 정보
=====================================

약 2,571,399 행만큼 정보를 저장하였으며, 해당 정보의 크기는 165.8MB
이다.

" 2019-11-24 \~ 2019-12-15 "

![](.//media/image24.png)

세부 내용은 위 Amazon RDS Mysql Schema를 참고하면 된다.

\[데이터 분석 결과\] Zeppelin Visualization
===========================================

1.  ![](.//media/image25.png)정류장 \[명지대(13195)\]의 배차간격을
    나타낸 표이다.\
    ![](.//media/image27.png)

그림11의 그래프 세부사항

2.  ![](.//media/image29.png)정류장 \[명지대(13194)\]의 배차간격을
    나타낸 표이다.\
    ![](.//media/image27.png)

그림12의 그래프 세부 사항

3.  ![](.//media/image32.png)![](.//media/image33.png)정류소\[명지대\]를 지나는 버스 노선의
    모든 정류장 평균 배차 간격

> ![](.//media/image27.png)
>
> 그림 13, 그림 14의 그래프 세부사항

\[데이터 분석 결과\] 결론
=========================

앞의 Visualization 된 결과를 통해 확인해보면 비교적 간격의 차이가 크지
않다. 평균 1분, 2분을 사이로 값이 변경되는데 평균적으로 1정거장에 걸리는
시간이 2분이라고 생각해보면 그렇다고 짧은 시간 또한 아니다.

결론을 도출하는 방법은 어렵지 않다. 값 중에서 평균 배차간격이 높은
시간을 교통 흐름이 혼잡한 것으로 전제를 잡았기 때문에, 배차간격이 길수록
해당 시간은 피하는 것이 맞다는 결론이다. 반대로 배차간격이 짧을수록 해당
시간은 이용하기 좋다는 결론을 내린다. 기준을 명확하게 내리기 위해
내림차순을 기준으로 상위 3위안에 포함된다면 피해야하는 시간으로, 하위
3위안에 포함된다면 좋은 시간으로 정의하겠다.

또한, 참고사항은 버스의 첫차 시간은 제각각 이기 때문에 첫차에 가까울수록
막차에 가까울수록 배차간격이 평균적으로 길 수 있다는 사실이다. 따라서
학생들이 점심 시간 이후부터 하교한다는 기준으로 학생들의 움직임이 있는
오후 1시 \~ 오후 11시를 평가 기준으로 지정하겠다.

1\. 정류장\[명지대(13195)\]에 관한 결론이다.

\[7017\]\[13195\]의 피해야하는 시간 : 17시, 18시,16시

\[7017\] \[13195\]의 좋은 시간 : 22시, 21시, 20시

\[7019\] \[13195\]의 피해야하는 시간 : 18시, 19시, 17시

\[7019\] \[13195\]의 좋은 시간 : 13시, 21시, 20시

\[7021\] \[13195\]의 피해야하는 시간 : 17시, 16시, 21시

\[7021\] \[13195\]의 좋은 시간 : 20시, 14시, 18시

\[7611\] \[13195\]의 피해야하는 시간 : 23시, 16시, 22시

\[7611\] \[13195\]의 좋은 시간 : 20시, 13시, 19시

\[7612\] \[13195\]의 피해야하는 시간 : 23시, 21시, 22시

\[7612\] \[13195\]의 좋은 시간 : 14시, 16시, 13시

\[7713\] \[13195\]의 피해야하는 시간 : 23시, 20시, 22시

\[7713\] \[13195\]의 좋은 시간 : 19시, 18시, 16시

\[7734\] \[13195\]의 피해야하는 시간 : 22시, 23시, 13시

\[7734\] \[13195\]의 좋은 시간 : 20시, 18시, 16시

2\. 정류장\[명지대(13194)\]에 관한 결론이다.

\[7017\]\[13194\]의 피해야하는 시간 : 14시, 17시, 19시.

\[7017\] \[13194\]의 좋은 시간 : 23시, 21시, 22시

\[7019\] \[13194\]의 피해야하는 시간 : 23시, 18시, 22시

\[7019\] \[13194\]의 좋은 시간 : 20시, 13시, 19시

\[7021\] \[13194\]의 피해야하는 시간 : 18시, 16시, 14시

\[7021\] \[13194\]의 좋은 시간 : 20시, 21시, 22시

\[7611\] \[13194\]의 피해야하는 시간 : 18시, 17시, 23시

\[7611\] \[13194\]의 좋은 시간 : 20시, 14시, 15시

\[7612\] \[13194\]의 피해야하는 시간 : 22시, 18시, 21시

\[7612\] \[13194\]의 좋은 시간 : 14시, 13시, 16시

\[7713\] \[13194\]의 피해야하는 시간 : 15시, 22시, 18시

\[7713\] \[13194\]의 좋은 시간 : 20시, 13시, 19시

\[7734\] \[13194\]의 피해야하는 시간 : 23시, 22시, 17시

\[7734\] \[13194\]의 좋은 시간 : 19시, 14시, 21시

3\. 전체적인 교통 흐름에 관한 결론이다. ( 밑줄은 13195 공통, 굵음은 13194
공통 )

\[7017\]의 피해야하는 시간 : **[17]{.underline}**시, [18]{.underline}시,
[16]{.underline}시.

\[7017\] 의 좋은 시간 : **23**시, **[22]{.underline}**시,
**[21]{.underline}**시

\[7019\] 의 피해야하는 시간 : **23**시, **[18]{.underline}**시,
[17]{.underline}시

\[7019\] 의 좋은 시간 : **[13]{.underline}**시, 12시, 14시

\[7021\] 의 피해야하는 시간 : [17]{.underline}시, **18**시,
**[16]{.underline}**시

\[7021\] 의 좋은 시간 : **[20]{.underline}**시, 23시, **21**시

\[7611\] 의 피해야하는 시간 : **17**시, **[23]{.underline}**시, **18**시

\[7611\] 의 좋은 시간 : **[20]{.underline}**시, [13]{.underline}시,
**14**시

\[7612\] 의 피해야하는 시간 : **18**시, **[22]{.underline}**시,
[23]{.underline}시

\[7612\] 의 좋은 시간 : **[13]{.underline}**시, **[14]{.underline}**시,
15시

\[7713\] 의 피해야하는 시간 : [23]{.underline}시,
**[22]{.underline}**시, 21시

\[7713\] 의 좋은 시간 : 17시, [16]{.underline}시, **13**시

\[7734\] 의 피해야하는 시간 : **[23]{.underline}**시,
**[22]{.underline}**시, **17**시

\[7734\] 의 좋은 시간 : [20]{.underline}시, **19**시, **21**시

최종 결론

-   7019의 18시는 피하세요

-   7019 는 13시에 이용하세요

-   7021은 16시는 피하세요

-   7021은 20시에 이용하세요

-   7611은 23시를 피하세요

-   7611은 20시에 이용하세요

-   7612는 22시를 피하세요

-   7612는 13시, 14시를 이용하세요

-   7713은 22시를 피하세요

-   7734는 23시, 22시를 피하세요

\[추가적인 확장 가능성\]
========================

현재 데이터는 "버스" 노선을 기준으로 맞춰져 있다. 하지만 해당 내용을 타
대중교통에 적용해도 전혀 문제가 없다. 예를 들어, 지하철 정보를
수집한다면 지하철이 연착되는 시간대를 조사하여 해당 시간대를 피하는
방법이 있다.

이러한 방법으로 응용하거나 또 다른 용도는 본 프로젝트의 WTIME 과
BUS\_IS="N"의 정보를 사용하는 것이다. 현재 데이터에도 BUS\_IS="N"일 때
WTIME이 입력되어 있다. 본 프로젝트에서는 사용되지 않았지만 BUS\_IS가 N인
값은 해당 정류장에서 다음 정류장까지 버스가 가는데 걸리는 시간이다. 이
값이 생성되게 된 이유는 특정 버스가 특정 정류장에 도착하면 BUS\_IS의
값은 "Y"로 변경된다. 그렇게 Mysql에 INSERT 하게 되고 INSERT하는 조건
자체가 BUS\_IS의 값 변동이 확인되면 추가하는 것으로 되어있기 때문에 해당
버스가 다음 정류장에 도착하게 되면 BUS\_IS의 값이 Y에서 N으로 되었을 때
또한 데이터를 INSERT 하게 된다. 예를 들어보면

INDEX\_NUM,BUS\_ID,STATION\_ID,TIME,BUS\_IS,WTIME

**300000,7019,13195,2019.12.16 13:00:00,N,120**

값이 있다. 이 값을 해석해보자면 300000번째 크롤링한 값이다. 7019번
버스는 정류장 13195부터 다음 정류장까지 걸린 시간이 120초가 걸렸다는
것이다.

이처럼, 해당 정류장부터 다음 정류장까지 걸린 시간 정보를 알 수 있다.
이를 이용해서 서울에 있는 650 여개의 데이터를 모두 크롤링하고 데이터를
갖고 있다고 가정하면 두가지 방향으로 나눠지는데 Real Time Data 분석을
통해 서울의 교통체증이 어느 정도인지 파악할 수 있다. 예를 들면, 특정
정류장부터 특정 정류장까지 평균보다 더 많은 시간이 소요된다면 해당
도로는 교통체증이 심한 상태인 것이다. 이 정보를 650개의 노선과 비교해
본다면, 해당 도로의 상황을 쉽고 빠르게 파악할 수 있을 것이다.
**요약하자면, 버스 배차간격을 통해 해당 도로의 교통상황을 정확하게
파악할 수 있다는 것이다.**

Real Time Data 분석 외에 다른 방법은 해당 도로의 교통체증이 어떤
시간대에 얼마나 걸릴지 예측해볼 수 있다. 심지어 버스가 실제로 운영한
시간대를 가져오기 때문에 보다 정확하게 파악할 수 있다.

\[참고사항\] 배차간격의 의문점
==============================

![](.//media/image36.png)![](.//media/image37.png)데이터 값을 확인해보면 이상하다고 느낄 수
있는 값이 있다.

5000이상의 값은 운영하지 않는 시간으로 인해 발생하는 오차 값이며
배차간격이 10초거나 10초후에 다음 버스가 도착하는 경우가 있는데 오류가
아닌 실제로 위 그림처럼 운행되는 일이 많음으로 그런 숫자가 발생하게 된
것이다.

\[웹 크롤러 파이썬 코드\]
=========================

[[https://github.com/Linho1150/HADOOP-BIG\_DATA\_PROJECT]{.underline}](https://github.com/Linho1150/HADOOP-BIG_DATA_PROJECT)

 \[데이터 분석 결과 -- zepl\]
=============================

[[https://www.zepl.com/viewer/notebooks/bm90ZTovL2hhaW5obzk1ODZAZ21haWwuY29tLzUzNzQ3OWY5ODllMDRjN2ZiNTFhZjViNTI3MGZmNWQzL25vdGUuanNvbg]{.underline}](https://www.zepl.com/viewer/notebooks/bm90ZTovL2hhaW5obzk1ODZAZ21haWwuY29tLzUzNzQ3OWY5ODllMDRjN2ZiNTFhZjViNTI3MGZmNWQzL25vdGUuanNvbg)

\[참고문헌\]
============

Asyncio 모듈 \[Asyncio 공식 레퍼런스\]

[[https://docs.python.org/ko/3/library/asyncio.html]{.underline}](https://docs.python.org/ko/3/library/asyncio.html)

Asyncio 모듈 \[asyncio : 단일 스레드 기반의 Nonblocking 비동기 코루틴
완전 정복\]

[[https://soooprmx.com/archives/6882]{.underline}](https://soooprmx.com/archives/6882)

Aiohttp 모듈\[Aiohttp 공식 레퍼런스\]

[[https://docs.python.org/ko/3/library/asyncio.html]{.underline}](https://docs.python.org/ko/3/library/asyncio.html)

Pymysql 모듈 예제 \[개발==삽질 tistory 블로그\]

[[https://sab-jil.tistory.com/6]{.underline}](https://sab-jil.tistory.com/6)

Sqoop user guide \[sqoop 공식 레퍼런스\]

[[https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html]{.underline}](https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html)

Sqoop use hive \[Dz zone\]

[[https://dzone.com/articles/sqoop-import-data-from-mysql-to-hive]{.underline}](https://dzone.com/articles/sqoop-import-data-from-mysql-to-hive)

Sqoop Overwrite \[cloudera.com\]

[[https://community.cloudera.com/t5/Support-Questions/In-Sqoop-import-is-there-an-option-to-overwrite-or-delete/td-p/221270]{.underline}](https://community.cloudera.com/t5/Support-Questions/In-Sqoop-import-is-there-an-option-to-overwrite-or-delete/td-p/221270)

Zeppelin -- hive 와 연동 \[cloudera.com\]

[[https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/using-zeppelin/content/using\_the\_jdbc\_interpreter\_to\_access\_hive.html]{.underline}](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.4/using-zeppelin/content/using_the_jdbc_interpreter_to_access_hive.html)

OOZIE \[권동섭 교수님,2018 OOZIE 교육자료\]
